(window.webpackJsonp=window.webpackJsonp||[]).push([[17],{480:function(a,s,t){"use strict";t.r(s);var e=t(28),c=Object(e.a)({},(function(){var a=this,s=a.$createElement,t=a._self._c||s;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("p",[a._v("环境：linux centos8")]),a._v(" "),t("h2",{attrs:{id:"scrapy"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scrapy"}},[a._v("#")]),a._v(" Scrapy")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[a._v("pip install Scrapy\n\nscrapy startproject spider_name  project_name \n")])])]),t("blockquote",[t("p",[a._v("spider_name ：为爬虫项目名称(子目录)")]),a._v(" "),t("p",[a._v("project_name ：为整个项目名称(上级目录)")])]),a._v(" "),t("h5",{attrs:{id:"单独执行命令"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#单独执行命令"}},[a._v("#")]),a._v(" 单独执行命令")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[a._v("scrapy crawl stock -o text.csv\n")])])]),t("blockquote",[t("p",[a._v("参数：-o")]),a._v(" "),t("p",[a._v("结果保存到 text.csv中")])]),a._v(" "),t("h2",{attrs:{id:"scrapyd"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scrapyd"}},[a._v("#")]),a._v(" scrapyd")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[a._v("pip install scrapyd\npip install scrapy-client\n")])])]),t("blockquote",[t("p",[a._v("理论只需要上面那个服务端，因为基本发布项目的办法有问题")])]),a._v(" "),t("h5",{attrs:{id:"在-project-name-目录下执行scrapyd"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#在-project-name-目录下执行scrapyd"}},[a._v("#")]),a._v(" 在 project_name 目录下执行scrapyd")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[a._v("scrapyd\n")])])]),t("h5",{attrs:{id:"修改-spider-name项目目录下，有一个scrapy-cfg的配置文件"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#修改-spider-name项目目录下，有一个scrapy-cfg的配置文件"}},[a._v("#")]),a._v(" 修改 spider_name项目目录下，有一个scrapy.cfg的配置文件")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[a._v("[settings]\ndefault = scrapy_stock.settings\n\n\n[deploy:demo]\nurl = http://localhost:6800/\nproject = scrapy_stock\n")])])]),t("h5",{attrs:{id:"demo是用于下面命令在scrapy-cfg找到对应的配置信息"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#demo是用于下面命令在scrapy-cfg找到对应的配置信息"}},[a._v("#")]),a._v(" demo是用于下面命令在scrapy.cfg找到对应的配置信息")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[a._v("scrapyd-deploy demo -p scrapy_stock\n")])])]),t("blockquote",[t("p",[a._v("scrapy-client发布任务版本")])]),a._v(" "),t("h5",{attrs:{id:"执行命令不需要用到demo"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#执行命令不需要用到demo"}},[a._v("#")]),a._v(" 执行命令不需要用到demo")]),a._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",[t("code",[a._v("curl http://localhost:6800/schedule.json -d project=scrapy_stock -d spider=stock\n")])])]),t("blockquote",[t("p",[a._v("spider:去scrapy_stock项目下找名为:stock的爬虫任务")])])])}),[],!1,null,null,null);s.default=c.exports}}]);